{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d8a23-cc51-4b75-a488-7ec928c8fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "def load_and_examine_data(file_path):\n",
    "   \n",
    "   \n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(\"Dataset Shape:\", df.shape)\n",
    "    print(\"\\nColumn Names:\")\n",
    "    print(df.columns.tolist())\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"\\nDescriptive Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def select_relevant_features(df):\n",
    "    \n",
    "    \n",
    "  \n",
    "    relevant_features = ['CustomerKey','Age', 'Gender', 'MaritalStatus','YearlyIncome','Education','Occupation','CommuteDistance','Region','NumberCarsOwned','TotalChildren',\n",
    "        'BikeBuyer'  \n",
    "    ]\n",
    "    \n",
    "   \n",
    "    available_features = [col for col in relevant_features if col in df.columns]\n",
    "    \n",
    "    print(\"Selected Features:\")\n",
    "    for feature in available_features:\n",
    "        print(f\"- {feature}\")\n",
    "    \n",
    "    return available_features\n",
    "\n",
    "def create_selected_dataframe(df, selected_features):\n",
    "\n",
    "    df_selected = df[selected_features].copy()\n",
    "    \n",
    "    print(f\"\\nNew DataFrame created with {len(selected_features)} features\")\n",
    "    print(f\"Shape: {df_selected.shape}\")\n",
    "    \n",
    "    return df_selected\n",
    "\n",
    "def determine_data_types(df_selected):\n",
    "    \n",
    "    data_types_analysis = {}\n",
    "    \n",
    "    for column in df_selected.columns:\n",
    "        dtype = df_selected[column].dtype\n",
    "        unique_values = df_selected[column].nunique()\n",
    "        sample_values = df_selected[column].dropna().head(10).tolist()\n",
    "        \n",
    "        # Determine data type category\n",
    "        if dtype in ['object']:\n",
    "            if unique_values == 2:\n",
    "                category = \"Discrete - Nominal (Binary)\"\n",
    "            else:\n",
    "                category = \"Discrete - Nominal\"\n",
    "        elif dtype in ['int64', 'int32'] and unique_values < 20:\n",
    "            category = \"Discrete - Ordinal/Nominal\"\n",
    "        elif dtype in ['int64', 'int32', 'float64', 'float32']:\n",
    "            if unique_values > 50:\n",
    "                category = \"Continuous - Ratio/Interval\"\n",
    "            else:\n",
    "                category = \"Discrete - Ordinal\"\n",
    "        else:\n",
    "            category = \"Unknown - Needs Investigation\"\n",
    "        \n",
    "        data_types_analysis[column] = {\n",
    "            'dtype': str(dtype),\n",
    "            'unique_values': unique_values,\n",
    "            'category': category,\n",
    "            'sample_values': sample_values\n",
    "        }\n",
    "    \n",
    "    print(\"\\nData Type Analysis:\")\n",
    "    print(\"-\" * 80)\n",
    "    for col, info in data_types_analysis.items():\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  - Data Type: {info['dtype']}\")\n",
    "        print(f\"  - Unique Values: {info['unique_values']}\")\n",
    "        print(f\"  - Category: {info['category']}\")\n",
    "        print(f\"  - Sample Values: {info['sample_values']}\")\n",
    "        print()\n",
    "    \n",
    "    return data_types_analysis\n",
    "\n",
    "def handle_null_values(df):\n",
    "    \n",
    "    print(\"Handling Null Values:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    for column in df_processed.columns:\n",
    "        null_count = df_processed[column].isnull().sum()\n",
    "        if null_count > 0:\n",
    "            print(f\"{column}: {null_count} null values\")\n",
    "            \n",
    "            if df_processed[column].dtype in ['object']:\n",
    "                # Categorical: fill with mode\n",
    "                mode_value = df_processed[column].mode()[0] if len(df_processed[column].mode()) > 0 else 'Unknown'\n",
    "                df_processed[column].fillna(mode_value, inplace=True)\n",
    "                print(f\"  -> Filled with mode: {mode_value}\")\n",
    "            else:\n",
    "                # Numerical: fill with median\n",
    "                median_value = df_processed[column].median()\n",
    "                df_processed[column].fillna(median_value, inplace=True)\n",
    "                print(f\"  -> Filled with median: {median_value}\")\n",
    "    \n",
    "    print(f\"\\nAfter handling nulls - Remaining null values: {df_processed.isnull().sum().sum()}\")\n",
    "    return df_processed\n",
    "\n",
    "def normalize_data(df, numerical_columns):\n",
    "   \n",
    "    print(\"\\nNormalizing Numerical Data:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    df_normalized = df.copy()\n",
    "    \n",
    "    # Min-Max Normalization (0-1 scaling)\n",
    "    scaler_minmax = MinMaxScaler()\n",
    "    df_normalized[numerical_columns] = scaler_minmax.fit_transform(df_normalized[numerical_columns])\n",
    "    \n",
    "    # Standardization (Z-score)\n",
    "    scaler_standard = StandardScaler()\n",
    "    standardized_data = scaler_standard.fit_transform(df[numerical_columns])\n",
    "    \n",
    "    # Create separate standardized dataframe\n",
    "    df_standardized = df.copy()\n",
    "    df_standardized[numerical_columns] = standardized_data\n",
    "    \n",
    "    print(\"Applied Min-Max Normalization and Z-score Standardization\")\n",
    "    \n",
    "    return df_normalized, df_standardized, scaler_minmax, scaler_standard\n",
    "\n",
    "def discretize_continuous_attributes(df, continuous_columns, n_bins=5):\n",
    "    \"\"\"\n",
    "    (c) Discretization (Binning) on continuous attributes\n",
    "    \"\"\"\n",
    "    print(f\"\\nDiscretizing Continuous Attributes into {n_bins} bins:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    df_discretized = df.copy()\n",
    "    \n",
    "    for column in continuous_columns:\n",
    "        if column in df_discretized.columns:\n",
    "            # Equal-width binning\n",
    "            df_discretized[f'{column}_binned'] = pd.cut(\n",
    "                df_discretized[column], \n",
    "                bins=n_bins, \n",
    "                labels=[f'{column}_bin_{i+1}' for i in range(n_bins)]\n",
    "            )\n",
    "            \n",
    "            # Equal-frequency binning\n",
    "            df_discretized[f'{column}_quantile'] = pd.qcut(\n",
    "                df_discretized[column], \n",
    "                q=n_bins, \n",
    "                labels=[f'{column}_q_{i+1}' for i in range(n_bins)],\n",
    "                duplicates='drop'\n",
    "            )\n",
    "            \n",
    "            print(f\"{column} -> {column}_binned, {column}_quantile\")\n",
    "    \n",
    "    return df_discretized\n",
    "\n",
    "def binarize_categorical_attributes(df, categorical_columns):\n",
    "    \"\"\"\n",
    "    (e) Binarization (One Hot Encoding) for categorical attributes\n",
    "    \"\"\"\n",
    "    print(\"\\nApplying One-Hot Encoding:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    for column in categorical_columns:\n",
    "        if column in df_encoded.columns:\n",
    "            # One-hot encoding\n",
    "            dummies = pd.get_dummies(df_encoded[column], prefix=column)\n",
    "            df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "            df_encoded.drop(column, axis=1, inplace=True)\n",
    "            print(f\"{column} -> {len(dummies.columns)} binary columns\")\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "# =============================================================================\n",
    "# PART III: PROXIMITY/CORRELATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def simple_matching_coefficient(x, y):\n",
    "    \n",
    "    matches = np.sum(x == y)\n",
    "    total = len(x)\n",
    "    return matches / total\n",
    "\n",
    "def jaccard_similarity_manual(x, y):\n",
    "    \n",
    "    intersection = np.sum((x == 1) & (y == 1))\n",
    "    union = np.sum((x == 1) | (y == 1))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def calculate_proximity_measures(df, obj1_idx=0, obj2_idx=1):\n",
    "    \n",
    "    print(f\"\\nCalculating Proximity Measures between objects {obj1_idx} and {obj2_idx}:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Get two objects (rows)\n",
    "    obj1 = df.iloc[obj1_idx].values\n",
    "    obj2 = df.iloc[obj2_idx].values\n",
    "    \n",
    "    # Ensure binary data for binary similarity measures\n",
    "    obj1_binary = (obj1 > 0).astype(int)\n",
    "    obj2_binary = (obj2 > 0).astype(int)\n",
    "    \n",
    "    # Simple Matching Coefficient\n",
    "    smc = simple_matching_coefficient(obj1_binary, obj2_binary)\n",
    "    print(f\"Simple Matching Coefficient: {smc:.4f}\")\n",
    "    \n",
    "    # Jaccard Similarity\n",
    "    jaccard = jaccard_similarity_manual(obj1_binary, obj2_binary)\n",
    "    print(f\"Jaccard Similarity: {jaccard:.4f}\")\n",
    "    \n",
    "    # Cosine Similarity\n",
    "    obj1_reshaped = obj1.reshape(1, -1)\n",
    "    obj2_reshaped = obj2.reshape(1, -1)\n",
    "    cosine = cosine_similarity(obj1_reshaped, obj2_reshaped)[0][0]\n",
    "    print(f\"Cosine Similarity: {cosine:.4f}\")\n",
    "    \n",
    "    return smc, jaccard, cosine\n",
    "\n",
    "def calculate_correlation(df, feature1='CommuteDistance', feature2='YearlyIncome'):\n",
    "\n",
    "    print(f\"\\nCalculating Correlation between {feature1} and {feature2}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if feature1 in df.columns and feature2 in df.columns:\n",
    "        # Pearson correlation\n",
    "        correlation, p_value = pearsonr(df[feature1], df[feature2])\n",
    "        print(f\"Pearson Correlation: {correlation:.4f}\")\n",
    "        print(f\"P-value: {p_value:.4f}\")\n",
    "        \n",
    "        # Spearman correlation (rank-based)\n",
    "        spearman_corr = df[feature1].corr(df[feature2], method='spearman')\n",
    "        print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "        \n",
    "        # Create correlation plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(df[feature1], df[feature2], alpha=0.6)\n",
    "        plt.xlabel(feature1)\n",
    "        plt.ylabel(feature2)\n",
    "        plt.title(f'Correlation between {feature1} and {feature2}')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(df[feature1], df[feature2], 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(df[feature1], p(df[feature1]), \"r--\", alpha=0.8)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return correlation, p_value, spearman_corr\n",
    "    else:\n",
    "        print(\"One or both features not found in dataset\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def main_pipeline(file_path):\n",
    "   \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ADVENTURE WORKS CYCLES - DATA MINING PIPELINE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # PART I: Feature Selection and Analysis\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PART I: FEATURE SELECTION AND ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load and examine data\n",
    "    df = load_and_examine_data(file_path)\n",
    "    \n",
    "    # Select relevant features\n",
    "    selected_features = select_relevant_features(df)\n",
    "    \n",
    "    # Create new dataframe with selected features\n",
    "    df_selected = create_selected_dataframe(df, selected_features)\n",
    "    \n",
    "    # Determine data types\n",
    "    data_types = determine_data_types(df_selected)\n",
    "    \n",
    "   \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PART II: DATA PREPROCESSING AND TRANSFORMATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Handle null values\n",
    "    df_no_nulls = handle_null_values(df_selected)\n",
    "  \n",
    "    numerical_cols = df_no_nulls.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df_no_nulls.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    numerical_cols = [col for col in numerical_cols if 'key' not in col.lower() and 'id' not in col.lower()]\n",
    "    \n",
    "    print(f\"\\nNumerical columns: {numerical_cols}\")\n",
    "    print(f\"Categorical columns: {categorical_cols}\")\n",
    "    \n",
    "   \n",
    "    if numerical_cols:\n",
    "        df_normalized, df_standardized, scaler1, scaler2 = normalize_data(df_no_nulls, numerical_cols)\n",
    "    else:\n",
    "        df_normalized = df_no_nulls.copy()\n",
    "        df_standardized = df_no_nulls.copy()\n",
    "    \n",
    "    \n",
    "    continuous_cols = [col for col in numerical_cols if df_no_nulls[col].nunique() > 10]\n",
    "    if continuous_cols:\n",
    "        df_discretized = discretize_continuous_attributes(df_normalized, continuous_cols)\n",
    "    else:\n",
    "        df_discretized = df_normalized.copy()\n",
    "    \n",
    "        if categorical_cols:\n",
    "        df_final = binarize_categorical_attributes(df_standardized, categorical_cols)\n",
    "    else:\n",
    "        df_final = df_standardized.copy()\n",
    "    \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PART III: PROXIMITY/CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    \n",
    "    if len(df_final) >= 2:\n",
    "        smc, jaccard, cosine = calculate_proximity_measures(df_final, 0, 1)\n",
    "    \n",
    "   \n",
    "    correlation_result = calculate_correlation(df_no_nulls)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return {\n",
    "        'original_data': df,\n",
    "        'selected_data': df_selected,\n",
    "        'processed_data': df_no_nulls,\n",
    "        'normalized_data': df_normalized,\n",
    "        'standardized_data': df_standardized,\n",
    "        'final_data': df_final,\n",
    "        'data_types': data_types\n",
    "    }\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
